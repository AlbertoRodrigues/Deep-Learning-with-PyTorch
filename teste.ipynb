{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from unidecode import unidecode\n",
    "import spacy.cli\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processo mais acurado at√© agora, modelo salvo localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo de portugu√™s\n",
    "nlp = spacy.load(\"modelos/pt_core_news_sm/pt_core_news_sm-3.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridade de Jaccard com lematiza√ß√£o: 0.333\n"
     ]
    }
   ],
   "source": [
    "def remover_acentos(texto):\n",
    "    return unidecode(texto)\n",
    "\n",
    "def preprocessar(texto):\n",
    "    # N√ÉO remova acentos antes do spaCy, para manter is_stop funcionando corretamente\n",
    "    doc = nlp(texto.lower())\n",
    "\n",
    "    lemas = [\n",
    "        remover_acentos(token.lemma_.lower())  # s√≥ remove acento ap√≥s lematiza√ß√£o\n",
    "        for token in doc\n",
    "        if not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    return set(lemas)\n",
    "\n",
    "def jaccard_lemmatizado(frase1, frase2):\n",
    "    set1 = preprocessar(frase1)\n",
    "    set2 = preprocessar(frase2)\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# üîé Exemplo de uso\n",
    "f1 = \"A casa √© azul\"\n",
    "f2 = \"A casa √© verde\"\n",
    "\n",
    "similaridade = jaccard_lemmatizado(f1, f2)\n",
    "print(\"Similaridade de Jaccard com lematiza√ß√£o:\", round(similaridade, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosseno (Bag of Words): 0.5\n",
      "Jaccard: 0.6\n"
     ]
    }
   ],
   "source": [
    "def cosine_bow_similarity(frase1, frase2):\n",
    "    vetorizar = CountVectorizer().fit_transform([frase1, frase2])\n",
    "    matriz = vetorizar.toarray()\n",
    "    similaridade = cosine_similarity(matriz)\n",
    "    return similaridade[0, 1]\n",
    "\n",
    "def jaccard_similarity(frase1, frase2):\n",
    "    set1 = set(frase1.lower().split())\n",
    "    set2 = set(frase2.lower().split())\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# Exemplos\n",
    "f1 = \"A casa √© azul\"\n",
    "f2 = \"A casa √© verde\"\n",
    "\n",
    "print(\"Cosseno (Bag of Words):\", round(cosine_bow_similarity(f1, f2), 3))\n",
    "print(\"Jaccard:\", round(jaccard_similarity(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Jaccard retirando stopwords e acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard (sem acentos e stopwords): 0.333\n"
     ]
    }
   ],
   "source": [
    "# Baixar stopwords se necess√°rio\n",
    "def remover_acentos(texto):\n",
    "    return unidecode(texto)\n",
    "\n",
    "def jaccard_sem_stopwords_acentos(frase1, frase2, idioma='portuguese'):\n",
    "    stop_words = set(remover_acentos(w.lower()) for w in stopwords.words(idioma))\n",
    "    \n",
    "    tokens1 = set(remover_acentos(palavra.lower()) for palavra in frase1.split() if remover_acentos(palavra.lower()) not in stop_words)\n",
    "    tokens2 = set(remover_acentos(palavra.lower()) for palavra in frase2.split() if remover_acentos(palavra.lower()) not in stop_words)\n",
    "    \n",
    "    intersecao = tokens1.intersection(tokens2)\n",
    "    uniao = tokens1.union(tokens2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# Exemplo\n",
    "f1 = \"A casa √© azul\"\n",
    "f2 = \"A casa √© verde\"\n",
    "\n",
    "print(\"Jaccard (sem acentos e stopwords):\", round(jaccard_sem_stopwords_acentos(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Jaccard retirando stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'azul', 'verde', 'casa'}\n",
      "Jaccard com remo√ß√£o de stopwords: 0.333\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity_sem_stopwords(frase1, frase2, idioma='portuguese'):\n",
    "    stop_words = set(stopwords.words(idioma))\n",
    "    \n",
    "    set1 = set(palavra.lower() for palavra in frase1.split() if palavra.lower() not in stop_words)\n",
    "    set2 = set(palavra.lower() for palavra in frase2.split() if palavra.lower() not in stop_words)\n",
    "    \n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    print(uniao)\n",
    "    return len(intersecao) / len(uniao)\n",
    "\n",
    "# Exemplo\n",
    "f1 = \"A casa √© azul\"\n",
    "f2 = \"A casa √© verde\"\n",
    "\n",
    "print(\"Jaccard com remo√ß√£o de stopwords:\", round(jaccard_similarity_sem_stopwords(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\anaconda3\\envs\\env_gen_ai\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21573691  0.18270384 -0.01125517  0.03685256 -0.01532448 -0.00880565\n",
      "  0.3670183   0.17374927 -0.0828202  -0.2361189 ]\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo de embedding\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Duas palavras similares\n",
    "palavra1 = \"carro\"\n",
    "palavra2 = \"gato\"\n",
    "\n",
    "# Gerar embeddings\n",
    "embedding1 = model.encode(palavra1)\n",
    "#embedding2 = model.encode(palavra2)\n",
    "\n",
    "# Calcular similaridade do cosseno\n",
    "#similaridade = cosine_similarity(\n",
    "#    [embedding1],\n",
    "#    [embedding2]\n",
    "#)[0][0]\n",
    "\n",
    "#print(f\"Similaridade entre '{palavra1}' e '{palavra2}': {similaridade:.4f}\")\n",
    "print(embedding1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando um modelo leve e eficaz\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02514462  0.02419465 -0.0337151   0.00544109 -0.05224889 -0.05682078\n",
      "  0.09118187  0.04109337 -0.03356854  0.01619318]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "# Palavra de exemplo\n",
    "palavra = \"intelig√™ncia\"\n",
    "\n",
    "# Gerando o embedding\n",
    "embedding1 = model.encode(palavra)\n",
    "\n",
    "# Mostrando as primeiras 10 dimens√µes\n",
    "print(embedding1[:10])\n",
    "print(embedding1.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
