{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\anaconda3\\envs\\env_gen_ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from unidecode import unidecode\n",
    "import spacy.cli\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "\n",
    "# Caminho onde o modelo está instalado\n",
    "#modelo_origem = spacy.util.get_package_path(\"pt_core_news_sm\")\n",
    "\n",
    "# Copie para o local desejado\n",
    "#shutil.copytree(modelo_origem, \"modelos/pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.cli.download(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "\n",
    "#print(os.path.exists(\"modelos/pt_core_news_sm/config.cfg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carrega o modelo de português\n",
    "nlp = spacy.load(\"modelos/pt_core_news_sm/pt_core_news_sm-3.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\anaconda3\\envs\\env_gen_ai\\lib\\site-packages\\pt_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "# Caminho completo onde o modelo está instalado\n",
    "#from spacy.util import get_package_path\n",
    "#print(get_package_path(\"pt_core_news_sm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_acentos(texto):\n",
    "    return unidecode(texto)\n",
    "\n",
    "def preprocessar(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    print(doc)\n",
    "    lemas = [\n",
    "        remover_acentos(token.lemma_)\n",
    "        for token in doc\n",
    "        if not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    return set(lemas)\n",
    "\n",
    "def jaccard_lemmatizado(frase1, frase2):\n",
    "    set1 = preprocessar(frase1)\n",
    "    set2 = preprocessar(frase2)\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# Exemplo\n",
    "f1 = \"A casa é azul\"\n",
    "f2 = \"A casa é verde\"\n",
    "\n",
    "print(\"Jaccard com lematização:\", round(jaccard_lemmatizado(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_bow_similarity(frase1, frase2):\n",
    "    vetorizar = CountVectorizer().fit_transform([frase1, frase2])\n",
    "    matriz = vetorizar.toarray()\n",
    "    similaridade = cosine_similarity(matriz)\n",
    "    return similaridade[0, 1]\n",
    "\n",
    "def jaccard_similarity(frase1, frase2):\n",
    "    set1 = set(frase1.lower().split())\n",
    "    set2 = set(frase2.lower().split())\n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# Exemplos\n",
    "f1 = \"A casa é azul\"\n",
    "f2 = \"A casa é verde\"\n",
    "\n",
    "print(\"Cosseno (Bag of Words):\", round(cosine_bow_similarity(f1, f2), 3))\n",
    "print(\"Jaccard:\", round(jaccard_similarity(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Jaccard retirando stopwords e acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar stopwords se necessário\n",
    "def remover_acentos(texto):\n",
    "    return unidecode(texto)\n",
    "\n",
    "def jaccard_sem_stopwords_acentos(frase1, frase2, idioma='portuguese'):\n",
    "    stop_words = set(remover_acentos(w.lower()) for w in stopwords.words(idioma))\n",
    "    \n",
    "    tokens1 = set(remover_acentos(palavra.lower()) for palavra in frase1.split() if remover_acentos(palavra.lower()) not in stop_words)\n",
    "    tokens2 = set(remover_acentos(palavra.lower()) for palavra in frase2.split() if remover_acentos(palavra.lower()) not in stop_words)\n",
    "    \n",
    "    intersecao = tokens1.intersection(tokens2)\n",
    "    uniao = tokens1.union(tokens2)\n",
    "    return len(intersecao) / len(uniao) if uniao else 0.0\n",
    "\n",
    "# Exemplo\n",
    "f1 = \"A casa é azul\"\n",
    "f2 = \"A casa é verde\"\n",
    "\n",
    "print(\"Jaccard (sem acentos e stopwords):\", round(jaccard_sem_stopwords_acentos(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade de Jaccard retirando stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_sem_stopwords(frase1, frase2, idioma='portuguese'):\n",
    "    stop_words = set(stopwords.words(idioma))\n",
    "    \n",
    "    set1 = set(palavra.lower() for palavra in frase1.split() if palavra.lower() not in stop_words)\n",
    "    set2 = set(palavra.lower() for palavra in frase2.split() if palavra.lower() not in stop_words)\n",
    "    \n",
    "    intersecao = set1.intersection(set2)\n",
    "    uniao = set1.union(set2)\n",
    "    print(uniao)\n",
    "    return len(intersecao) / len(uniao)\n",
    "\n",
    "# Exemplo\n",
    "f1 = \"A casa é azul\"\n",
    "f2 = \"A casa é verde\"\n",
    "\n",
    "print(\"Jaccard com remoção de stopwords:\", round(jaccard_similarity_sem_stopwords(f1, f2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo de embedding\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Duas palavras similares\n",
    "palavra1 = \"carro\"\n",
    "palavra2 = \"gato\"\n",
    "\n",
    "# Gerar embeddings\n",
    "embedding1 = model.encode(palavra1)\n",
    "#embedding2 = model.encode(palavra2)\n",
    "\n",
    "# Calcular similaridade do cosseno\n",
    "#similaridade = cosine_similarity(\n",
    "#    [embedding1],\n",
    "#    [embedding2]\n",
    "#)[0][0]\n",
    "\n",
    "#print(f\"Similaridade entre '{palavra1}' e '{palavra2}': {similaridade:.4f}\")\n",
    "print(embedding1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Carregando um modelo leve e eficaz\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavra de exemplo\n",
    "palavra = \"inteligência\"\n",
    "\n",
    "# Gerando o embedding\n",
    "embedding1 = model.encode(palavra)\n",
    "\n",
    "# Mostrando as primeiras 10 dimensões\n",
    "print(embedding[:10])\n",
    "print(embedding.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
